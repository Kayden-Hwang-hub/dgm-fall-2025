---
layout: distill
title: "Lecture 15"
description: "A Linear Intro to Generative Models"
date: 2025-10-27

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Kyoheon Hwang
  - name: Jan Rovirosa Illa
  - name: Lynn Huang

editors:
  - name: "-"
---


## Today's Topics:
- [1. Generative Models](#1-generative-models)
- [2. Generative and Discriminative Models](#2-generative-and-discriminative-models)
- [3. Example Discriminative Model:Logistic Regression](#3-example-discriminative-model-logistic-regression)
- [4. Example Generative Model:Naive Bayes](#4-example-generative-model-naive-bayes)
- [5. Discriminative vs Generative Models](#5-discriminative-vs-generative-models)
- [6. Logistic Regression vs Naive Bayes](#6-logistic-regression-vs-naive-bayes)
- [7. Modern DGMs](#7-modern-dgms)

---

## 1.Generative Models

### Deep Generative Models (DGM)

**Key Characteristics**
Previoulsy, we learned about discriminative model that can classify images (e.g., CNN).
Now, we will learn generative models that not only classify images, but they can also generate them on their own - such as ChatGPT, Gemini or DeepSeek.

<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure1.png' | relative_url }}" width="550">
  <figcaption>Figure 1. Difference between discriminative and generative model</figcaption>
</figure>


<br>



---
## 2. Generative and Discriminative Models

### More detail about difference between two Models
**1. Generative Models**
  - Models focus on learning the underlying distribution of the data
  - aim to understand how the data is generated, so allows them to create new data
  - Models the joint distribution P(X,Y) where X represents the features Y denotes the class labels

**2. Descriminative Models**
  - Models focus on modeling and decision boundary between classes
  - learn conditional distribution P(Y|X) which represents the probability of a class label given the input features 
  - used for classification tasks or distinguishing between classes based on features
  
<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure2.png' | relative_url }}" width="550">
  <figcaption>Figure 2. entire data distribution vs conditional probability </figcaption>
</figure>


<br>



---
**3. Two paths to P(Y|X)**

**Descriminative Models**
  - Direct path
  - When you observe X, Y (data) then you directly learn P(Y|X) which is the boundary between classes - that is, "Given X(data), what is the most likely Y(label)
  - In classification, $\hat{Y} = argmax_Y P(Y \mid X)$

**Generative Models**
  - Observe (X, Y)
  - you first learn:
    - $P(X \mid Y)$ → how data X is distributed within each class
    - $P(Y)$ → Distribution of Y (the prior)
  - also you can calculate $P(X) = \int_{Y} P(X,Y) \ dY$ which is the marginal probability of X regardless of Y
  - finally we yield $P(Y \mid X)$ using bayes' rule
    - $P(Y \mid X) = \frac{P(X \mid Y) \, P(Y)}{P(X)}$
  - In classification task, we don't need to calculate $P(X)$
    - $\hat{Y} = argmax_Y P(X \mid Y) \ P(Y)$

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure3.png' | relative_url }}" width="550">
  <figcaption>Figure 3. Path to P(Y|X)</figcaption>
</figure>


<br>



---

## 3. Example Discriminative Model:Logistic Regression
**Core Characteristic of Discriminative Model:** Parameterization
- Parameterization in Logistic Regression
  - $P(Y = 1 \mid X) = \sigma(\theta^{T} \ X)$, where $\sigma(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function
  - $P(Y = 0 \mid X) = 1 - P(Y=1 \mid X)$
  - "Why do we choose this form of parameterization?"
  - Because in logistic regression, we **assume the log-odds of the probability is a linear function of the input**
<latex-js style="border: 1px dashed #aaa;">

$$
\log\frac{P(Y = 1 \mid X)}{P(Y = 0 \mid X)} = \log\frac{\sigma(\theta^{T} \ X)}{1 - \sigma(\theta^{T} \ X)}
\=  \log\frac{\frac{1}{1+e^{-\theta^{T} \ X}}}{1 - \frac{1}{1+e^{-\theta^{T} \ X}}}
$$

$$
\= \log\frac{\frac{1}{1+e^{-\theta^{T} \ X}}}{\frac{e^{-\theta^{T} \ X}}{1+e^{-\theta^{T} \ X}}}
$$

\begin{itemize}
\item In logistic regression the log odds is a linear combination of the features X
\end{itemize}

</latex-js>

**Key Mechanism:**
- Replace traditional matrix multiplication with a convolution operation.
- This concept can be extended beyond images to graph-structured data.



### Weight Sharing in Kernels
***How it works:*** A kernel (a small matrix of weights, e.g., $w, x, y, z$) acts as a sliding filter.
- Parameter Re-use: These weights are reused at every position as the filter slides over the input.
- Output Generation: The output (part of a "feature map") is calculated by applying the kernel's weights to the corresponding input values at each position (e.g., $aw + bx + ey + fz$).


<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure6.png' | relative_url }}" width="550">
  <figcaption>Figure 6. Example of Weight Sharing</figcaption>
</figure>

<br>

### Alternative Visualization of Kernels
**Concept:** A "feature detector" (the filter or kernel) slides over the input image (like the "5") to generate a "feature map".
- Receptive Field: The specific patch of pixels the kernel is looking at at any given moment is called the "receptive field".
- Calculation: Each value in the "feature map" is computed as a weighted sum (dot product) of the kernel's weights ($w_i$) and the pixel values ($x_i$) within its receptive field: $\sum w_i x_i$.
- Guiding Principle: A feature detector that works well in one region (e.g., detects a curve) may also work well in another region.


<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure7.png' | relative_url }}" width="550">
  <figcaption>Figure 7. Example of Visualization of Kernels</figcaption>
</figure>

### Kernels for each channel
Multiple *feature detectors* (kernels) can be applied in parallel to the same input image.  
Each kernel learns different weights, producing different **feature maps** that highlight distinct aspects of the image.

- Each channel of the input (e.g., R/G/B for color images) has its own set of kernel weights.  
- The outputs across channels are summed to produce the final feature map.  
- This allows the network to combine local information across multiple input channels.  
- **Key idea:** sparse connectivity and weight sharing are still preserved, even with multiple kernels.

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure8.png' | relative_url }}" width="550">
  <figcaption>Figure 8. Kernels applied to each channel create multiple feature maps.</figcaption>
</figure>

### Convolutional Neural Networks [LeCun 1989]
LeCun and colleagues pioneered the use of Convolutional Neural Networks (CNNs) for digit recognition.  
Their architecture, known as **LeNet-5**, combined convolutional feature extraction with traditional fully connected classifiers.

**Key insights:**
- **Automatic feature extraction:** Convolution + subsampling layers automatically learn and compress spatial features from raw pixel input.  
- **Regular classifier at the end:** Fully connected layers + output layer act like a standard neural net classifier.  
- **Weight sharing:** Filters (kernels) are reused across the input, drastically reducing the number of parameters.  
- **Pooling (subsampling):** Early versions used average pooling to downsample feature maps and add robustness to shifts.  
- **Hierarchical learning:** Local patterns → combined into higher-level features → final object recognition.

**Network structure (LeNet-5 example):**
- Input: 32×32 image.  
- C1: Convolutional layer → 6 feature maps of size 28×28.  
- S2: Subsampling (pooling) layer → 6 maps of size 14×14.  
- C3: Convolutional layer → 16 maps of size 10×10.  
- S4: Subsampling layer → 16 maps of size 5×5.  
- C5: Convolutional layer → 120 feature maps of size 1×1.  
- F6: Fully connected layer → 84 units.  
- Output: 10-way classifier with Gaussian (later replaced by softmax).


<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure9.png' | relative_url }}" width="550">
  <figcaption>Figure 9. LeNet-5 architecture (LeCun et al., 1989/1998). Convolutions and pooling serve as automatic feature extractors, followed by fully connected layers for classification.</figcaption>
</figure>

### "Pooling": lossy compression
Pooling layers reduce the dimensionality of feature maps while retaining the most important information.  
This introduces **translation invariance**, helps control overfitting, and lowers computational cost.

**Types of pooling:**
- **Max pooling:** selects the maximum value from each region.  
- **Mean (average) pooling:** computes the average value from each region.  

**Key points:**
- Pooling operates over small, non-overlapping windows (e.g., 3×3).  
- The stride determines how the window moves across the input (e.g., stride = (3,3)).  
- Some information is discarded in the process, which is why pooling is considered **lossy compression**.  


<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure10.png' | relative_url }}" width="550">
  <figcaption>Figure 10. Pooling operations: max pooling preserves the strongest activation, while mean pooling averages across the region.</figcaption>
</figure>

### Main ideas of CNNs
Convolutional Neural Networks are built around three core principles:

- **Sparse connectivity:**  
  - Each element in the feature map is connected only to a small patch of the input (the receptive field).  
  - This is very different from fully connected networks, where each output is connected to all inputs.  
  - **Key point:** don't connect everything to everything — only local regions are connected.  

- **Parameter sharing:**  
  - The same weights (a kernel/filter) are reused for different patches of the input.  
  - This reduces the number of parameters and enforces translation invariance (the same feature can be detected anywhere in the input).  

- **Many layers:**  
  - Stacking multiple convolution and pooling layers reduces the spatial size of the representation.  
  - Local features are gradually combined into global patterns.  
  - Finally, a **multilayer perceptron (MLP)** is placed on top to map the compact feature representation into predictions (e.g., class labels).
 
---

## 4. CC vs Convolution

### Convolution: Adding two random variables
Convolution also appears in probability theory, when adding two independent random variables.

- Suppose $X \sim P_X, \; Y \sim P_Y$ are independent.  
- What is $P(X+Y=z)$?


**Continuous case:**
$$
P(X+Y=z) = \int P_X(x) P_Y(z-x) \, dx
$$

This integral is called the **convolution** of $P_X$ and $P_Y$:  
$$
(P_X * P_Y)(z) = \int P_X(x) P_Y(z-x)\, dx
$$

For discrete random variables, convolution is defined using a summation:

$$
(P_X * P_Y)(z) = \sum_x P_X(x) P_Y(z-x)
$$

More generally:  
- **Discrete:** $P_{X+Y}(z) = \sum_x P_{X,Y}(x, z-x)$  
- **Continuous:** $f_{X+Y}(z) = \int f_{X,Y}(x, z-x)\, dx$

In CNNs, convolution is not about probability, but the **same mathematical operation** is reused.  

- A kernel (filter) slides across the activation window.  
- At each location, the kernel performs a weighted sum (dot product) of the overlapping region.  

Formally:
$$
Z[i,j] = \sum_{u=-k}^k \sum_{v=-k}^k K[u,v] \, A[i-u, j-v]
$$

Which is written compactly as:
$$
Z[i,j] = K * A
$$

### Cross-Correlation vs. Convolution
In practice, most deep learning libraries (e.g., PyTorch, TensorFlow) actually implement **cross-correlation** rather than strict convolution.

- **Cross-correlation:**
$$
Z[i,j] = \sum_{u=-k}^k \sum_{v=-k}^k K[u,v] \, A[i+u, j+v]
$$

- **Convolution:**
$$
Z[i,j] = \sum_{u=-k}^k \sum_{v=-k}^k K[u,v] \, A[i-u, j-v]
$$

**Key difference:**  
- Convolution flips the kernel horizontally and vertically before sliding.  
- Cross-correlation uses the kernel as-is.

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure11.png' | relative_url }}" width="5500">
  <figcaption>Figure 11. Cross-correlation vs. convolution. Convolution involves flipping the kernel, while CNNs usually use cross-correlation.</figcaption>
</figure>

### Sparse Connectivity in CNNs
Unlike fully-connected layers, CNNs use **local connectivity**:  
- Each neuron in the feature map connects only to a **small patch** of the input (defined by the kernel size).  
- This drastically reduces the number of parameters compared to dense connections.  
- Sparse connections also enforce the idea that **local patterns** (e.g., edges, corners) are important for building up more complex representations.

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure12.png' | relative_url }}" width="550">
  <figcaption>Figure 12. CNNs use sparse connectivity (top) compared to dense fully-connected layers (bottom).</figcaption>
</figure>

### Receptive Fields
Although each neuron starts with a **local receptive field**, stacking multiple convolution layers expands the effective receptive field:  
- Lower layers capture **small/local features** (edges, corners).  
- Higher layers combine these into **larger/more abstract features** (shapes, objects).  
- This hierarchical growth enables CNNs to represent both local and global patterns.

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure13.png' | relative_url }}" width="550">
  <figcaption>Figure 13. Receptive fields grow with depth: higher-layer neurons aggregate information from larger input regions.</figcaption>
</figure>

### Impact of convolutions on size

The side length O of the feature map is below:
$$
O = \frac{W-K+2P}{S}+1
$$
W: input width. K: kernel width. P: padding. S: stride

Below is a graph showing padding and stride:

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure14.png' | relative_url }}" width="550">
  <figcaption>Figure 14. Padding.</figcaption>
</figure>

### Kernel dimensions and trainable parameters

- For example, a convolutional layer has 3 input channels, 8 kernels, a kernel size of 5 × 5, and a stride of (1,1). Each kernel has dimensions 5 × 5, so the total number of trainable parameters is 5 × 5 × 3 × 8.

- For a 28 × 28 input image, using a convolutional layer with a 5 × 5 kernel and stride of 1, the output size will be 24 × 24.

### CNNs and Translation/Rotation/Scale Invariance

- CNNs are not inherently invariant to translation, rotation, or scale. Consequently, applying data augmentation by shifting, rotating, or scaling images can make CNN more robust during CNN training.

## 5. CNN Backprop

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure15.png' | relative_url }}" width="550">
  <figcaption>Figure 15. Computational Graph.</figcaption>
</figure>

- In the computation graph, x: input. w: kernel weights. a: activation output. o: fully connected layer output. l: loss function.
- By $y = X * W + b$ we can derive that $\frac{\partial{L}}{\partial{W}} = X * \frac{\partial{L}}{\partial{y}}$
- the operator * means the sum of multiplication of two matrice's corresponding entry.

## mean of gradient

- For each kernel, there are multiple receptive fields in the input. For instance, a 3 × 3 kernel applied to a 5 × 5 input produces 4 receptive fields. During backpropagation, we compute the gradient for each receptive field and then take the average to update the kernel weights.
$$
w = w - \eta*\frac{\frac{l}{w}_1+\frac{l}{w}_2+\frac{l}{w}_3+\frac{l}{w}_4}{4}
$$

## 6. CNN PyTorch

- in_channels: 1 for grayscale images, 3 for RGB images.

- nn.Sequential: a container that holds all layers of the model; can often be used instead of explicitly defining a forward() function.

- nn.Conv2d: a two-dimensional convolutional layer.

- nn.MaxPool2d: a two-dimensional max pooling layer.

