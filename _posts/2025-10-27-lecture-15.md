---
layout: distill
title: "Lecture 15"
description: "A Linear Intro to Generative Models"
date: 2025-10-27

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Kyoheon Hwang
  - name: Jan Rovirosa Illa
  - name: Lynn Huang

editors:
  - name: "-"
---


## Today's Topics:
- [1. Generative Models](#1-generative-models)
- [2. Generative and Discriminative Models](#2-generative-and-discriminative-models)
- [3. Example Discriminative Model:Logistic Regression](#3-example-discriminative-model-logistic-regression)
- [4. Example Generative Model:Naive Bayes](#4-example-generative-model-naive-bayes)
- [5. Discriminative vs Generative Models](#5-discriminative-vs-generative-models)
- [6. Logistic Regression vs Naive Bayes](#6-logistic-regression-vs-naive-bayes)
- [7. Modern DGMs](#7-modern-dgms)

---

## 1.Generative Models

### Deep Generative Models (DGM)

**Key Characteristics**
  - Previoulsy, we learned about discriminative model that can classify images (e.g., CNN).
  - Now, we will learn generative models that not only classify images, but they can also generate them on their own - such as ChatGPT, Gemini or DeepSeek.

<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure1.png' | relative_url }}" width="550">
  <figcaption>Figure 1. Difference between discriminative and generative model</figcaption>
</figure>


<br>



---


## 2. Generative and Discriminative Models

### More detail about difference between two Models
**1. Generative Models**
  - Models focus on learning the underlying distribution of the data
  - aim to understand how the data is generated, so allows them to create new data
  - Models the joint distribution $P(X,Y)$ where $X$ represents the features Y denotes the class labels

**2. Descriminative Models**
  - Models focus on modeling and decision boundary between classes
  - learn conditional distribution $P(Y \mid X)$ which represents the probability of a class label given the input features 
  - used for classification tasks or distinguishing between classes based on features
  
<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure2.png' | relative_url }}" width="550">
  <figcaption>Figure 2. entire data distribution vs conditional probability </figcaption>
</figure>


<br>



---
**3. Two paths to P(Y|X)**

**Descriminative Models**
  - Direct path
  - When you observe X, Y then you directly learn $P(Y \mid X)$ which is the boundary between classes
  - $P(Y \mid X)$ is, "Given X(data), what is the most likely Y(label)
  - In classification, $\hat{Y} = argmax_Y P(Y \mid X)$

**Generative Models**
  - Observe (X, Y)
  - you first learn:
    - $P(X \mid Y)$ → how data X is distributed within each class
    - $P(Y)$ → Distribution of Y (the prior)
  - also you can calculate $P(X) = \int_{Y} P(X,Y) \ dY$ which is the marginal probability of X regardless of Y
  - finally we yield $P(Y \mid X)$ using bayes' rule
    - $P(Y \mid X) = \frac{P(X \mid Y) \, P(Y)}{P(X)}$
  - In classification task, we don't need to calculate $P(X)$
    - $\hat{Y} = argmax_Y P(X \mid Y) \ P(Y)$

<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure3.png' | relative_url }}" width="550">
  <figcaption>Figure 3. Path to P(Y|X)</figcaption>
</figure>


<br>



---


## 3. Example Discriminative Model:Logistic Regression
**Core Characteristic of Discriminative Model:** Parameterization
- Parameterization in Logistic Regression
  - $P(Y = 1 \mid X) = \sigma(\theta^{T} \ X)$, where $\sigma(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function
  - $P(Y = 0 \mid X) = 1 - P(Y=1 \mid X)$
  - "Why do we choose this form of parameterization?"
  - Because in logistic regression, we **assume the log-odds of the probability is a linear function of the input**
<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure5.png' | relative_url }}" width="550">
</figure>
   - In logistic regression the log odds is a linear combination of the features X
   - Estimate $\hat{\theta}$ form observation:
<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure4.png' | relative_url }}" width="550">
</figure>




<br>



---



---

## 4. Example Generative Model: Naive Bayes
**Core Characteristic of Generative Model:**

We observe X and Y. Then we learn $P(X \mid Y) and P(Y)$ and we use it to derive $P(Y \mid X) = \frac{P(X \mid Y) P(Y)}{P(X)}$, where $P(Y = 1) = \frac{number \hspace{1em} of \hspace{1em} samples \hspace{1em} with \hspace{1em} Y = K}{Total \hspace{1em} samples}$
- Parameterize
  - Assume $P(X \mid Y) = \prod_{j=1}^{d} P(X_j \mid Y)$
  - $P(X_j \mid Y) = N (\mu_{jk} , \sigma_{jk}^{2})$
- Estimate
  - $\hat{\mu} , \hat{\sigma} = argmax_{\mu , \sigma} P(X \mid Y)$
- Calculate
  - $P(Y=1 \mid X) = \frac{\prod_{j=1}^{d} P(X_j \mid Y) P(Y=1)}{P(X)}$
 
## MAP / Regularization Note:

Logistic regression: 
We change our $\hat{\theta}$ estimates from observations by:

<figure>
  <img src="{{ '/assets/img/notes/lecture-15/figure6.png' | relative_url }}" width="550">
</figure>

## 4. Discriminative vs Generative Models

- Discriminative models optimize the conditional likelihood:
$\hat{\theta_{disc}} = argmax_{\theta} P(Y \mid X; \theta) = argmax_{\theta} \frac{P(X \mid Y ; \theta) P(Y; \theta)}{P(X; \theta)}$
- Generative models optimize the joint likelihood:
$\hat{\theta_{disc}} = argmax_{\theta} P(X, Y; \theta) = argmax_{\theta} P(X \mid Y ; \theta) P(Y; \theta)$

  This means they are exactly the same optimization when $P(X; \theta)$ is invariant to $\theta$
