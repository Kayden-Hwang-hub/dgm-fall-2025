---
layout: distill
title: "Lecture 13"
description: "Convolutional Neural Networks"
date: 2025-10-15

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Che Tian
  - name: Zhiyuan Li
  - name: Siti Aleeya Nuha Binti Roslee

editors:
  - name: "Editor1"
---


## Today's Topics:
- [1. What CNNs Can Do](#1-what-cnns-can-do)
- [2. Image Classification](#2-image-classification)
- [3. CNN Basics](#3-cnn-basics)
- [4. Cross-Correlation vs Convolution](#4-cc-vs-convolution)
- [5. CNNs & Backpropagation](#5-cnn-backprop)
- [6. CNNs in PyTorch](#5-cnn-pytorch)

---

## 1. What CNNs Can Do

### Image Classification
**Definition:** Assigning a single label to an entire image from a set of predefined categories.  

**Key Characteristics:**
- Input: Entire image
- Output: Single class label with confidence score
- Question Answered: "What is the main object in this image?"

**Examples:**

- Classifying an image as "cat" vs "dog"
- Identifying handwritten digits (0-9)
- Medical image diagnosis (normal vs abnormal)

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure1.png' | relative_url }}" width="550">
  <figcaption>Figure 1. Example of Image Classification</figcaption>
</figure>

<br>

### Object Detection
**Definition:** Identifying and localizing multiple objects within an image, including their positions and classifications.

**Key Characteristics:**
- Input: Entire image
- Output: Multiple bounding boxes + class labels
- Question Answered: "What objects are where in this image?"

**Examples:**
- Detecting cars, trucks, pedestrians in autonomous driving
- Finding faces and facial features in photos
- Inventory counting in retail

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure2.png' | relative_url }}" width="550">
  <figcaption>Figure 2. Example of Object Detection</figcaption>
</figure>

<br>

### Object Segmentation
**Definition:** Pixel-level classification where each pixel in the image is assigned to a specific object category.

**Key Characteristics:**
- Input: Entire image
- Output: Pixel-wise mask with class labels
- Question Answered: "Which pixels belong to which objects?"

**Types of Segmentation:**
1. Semantic Segmentation:
   - Labels each pixel with a class
   - Doesn't distinguish between object instances
   - Example: All "car" pixels get same label

2. Instance Segmentation:
   - Identifies and separates individual object instances
   - Example: Differentiates between car1, car2, car3

**Examples:**
- Medical imaging (tumor boundary detection)
- Autonomous driving (road, pedestrian, vehicle segmentation)
- Photo editing tools (background removal)

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure3.png' | relative_url }}" width="550">
  <figcaption>Figure 3. Example of Object Segmentation</figcaption>
</figure>

---

## 2. Image Classification

### Why Images Are Hard for Neural Networks
**1. Visual Variations**
  - Lighting changes: Same object under different illumination
  - Contrast differences: Varying color and brightness levels
  - Viewpoint variations: Objects seen from different angles
  - Background clutter: Objects in complex environments

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure4.png' | relative_url }}" width="550">
  <figcaption>Figure 4. Example of Image Classification Challenge</figcaption>
</figure>

<br>

**2. The Limitations of Fully-Connected Networks**
  - For a 3×200×200 RGB image: 120,000 input pixels. Each neuron in first hidden layer needs 120,000 weights
  - Multiple hidden layers → parameter explosion

<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure5.png' | relative_url }}" width="550">
  <figcaption>Figure 5. Example of Limitation of Fully-Connected Networks</figcaption>
</figure>

---

## CNN Basics
### Convolutional Neural Networks (LeCun 1989)
**Core Idea:** Parameter Sharing
- Instead of learning weights specific to absolute positions, learn weights defined for relative positions.
- Learn "filters" that are reused across the entire image.
- This allows the network to generalize across spatial translation of the input (e.g., recognize an object whether it's in the top-left or bottom-right).


**Key Mechanism:**
- Replace traditional matrix multiplication with a convolution operation.
- This concept can be extended beyond images to graph-structured data.



### Weight Sharing in Kernels
***How it works:*** A kernel (a small matrix of weights, e.g., $w, x, y, z$) acts as a sliding filter.
- Parameter Re-use: These weights are reused at every position as the filter slides over the input.
- Output Generation: The output (part of a "feature map") is calculated by applying the kernel's weights to the corresponding input values at each position (e.g., $aw + bx + ey + fz$).


<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure6.png' | relative_url }}" width="550">
  <figcaption>Figure 6. Example of Weight Sharing</figcaption>
</figure>

<br>

### Alternative Visualization of Kernels
**Concept:** A "feature detector" (the filter or kernel) slides over the input image (like the "5") to generate a "feature map".
- Receptive Field: The specific patch of pixels the kernel is looking at at any given moment is called the "receptive field".
- Calculation: Each value in the "feature map" is computed as a weighted sum (dot product) of the kernel's weights ($w_i$) and the pixel values ($x_i$) within its receptive field: $\sum w_i x_i$.
- Guiding Principle: A feature detector that works well in one region (e.g., detects a curve) may also work well in another region.


<figure>
  <img src="{{ '/assets/img/notes/lecture-13/figure7.png' | relative_url }}" width="550">
  <figcaption>Figure 7. Example of Visualization of Kernels</figcaption>
</figure>

Parameter sharing: As one kernel/filter slide across the input matrix, its weights remains unchanged. For example, we have four 3*3 kernel. Then the number of parameters(weights) is 3*3*4=36. 

## 5. CNNs & Backpropagation
### computation graph:
<figure>
  <img src="{{ '/assets/img/notes/lecture-13/computation.png' | relative_url }}" width="550">
  <figcaption>Figure 7. Example of Visualization of Kernels</figcaption>
</figure>
x: input. w: kernel weights. a: activation output. o: fully connected layer output. l: loss function.
By
$$
y = X * W + b
$$
we can derive that
$$
\frac{\partial{L}}{\partial{W}} = X * \frac{\partial{L}}{\partial{y}}
* means the sum of multiplication of two matrice's corresponding entry.

### mean of gradient
Since for one kernel, there are many receptive fields from the input(e.g., a 3*3 kernel has 4 receptive fields for a 5*5 image input). We need to take average of the gradients for each gradient derived from each receptive fields. 
$
w = w - \eta\frac{\frac{l}{w}_1+\frac{l}{w}_2+\frac{l}{w}_3+\frac{l}{w}_4}{4}
$


## 6. CNN in PyTorch
in_channels: 1 when gray image, 3 when RGB image.
nn.Sequential: contains every layer of the model. A better substitution of the forward() function.
nn.Conv2d: two dimensional convolutional layer.
nn.MaxPool2d: two dimensional max pooling layer.


